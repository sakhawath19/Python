### Basic settings for jupyter notebook  ###############################################################

plt.style.use('fivethirtyeight')				# Choose any plotting style which you like, for example seaborn

pd.set_option('display.width', 100)			# Set how many columns you want to see in your window

pd.set_option('precision', 3)				# Number of digit to be precise





### Data loading ##################################################################################	
				
df = pd.read_csv('./input/data.csv', index_col='name', na_values=['NAN', 'Missing'])
						# Replacing custom missing values by NaN during loading data





### Data cleaning #################################################################################

- NaN/np.nan, None are system NaN values

- NA, Missing, MISSING custom NaN values # Replace those NaN or any other values 

- df.info()						# Observe missing values in each attribute

- df.count()					# Non null values in each attribute

- df.isnull().sum()					# Missing values in each attribute

- df.isnull().count()					# Valid + null values  

- df.isna()						# Boolean results for NaN values

- df.method(inplace = True)				# Applied any method in df with inplace=True will change the original df
						# Initially try to use any method without inplace and see the result


- Look at Bookmarks/Kaggle/how to handle missing data 

- df.dropna(inplace = True)				# Drop rows with any missing values; usually not recommended
						
- df.dropna(axis='index', how='any')			# Delete rows if any values are NaN

- df.dropna(how ='all',inplace = True)			# Drop rows if all the values are null 

- df.dropna(thresh=4, inplace=True)			# Delete rows that has more than 4 missing values 

- df.dropna(axis=1, inplace=True)				# Delete features that has some missing values

- df.dropna(axis='index', how='all', subset=['col1', 'col2'])	# Delete rows if both columns have NaN values
				

- df.dropna(axis='index', how='any', subset=['col1', 'col2'])	# Delete rows if any column has NaN value
				

- df.replace(['NA', 'Missing'], np.nan, inplace=True)		# Replace custom missing values by np.nan. 
						# System can deal with np.nan. It will consider as missing value
				

The only case that it may worth deleting a variable is when its missing values are more than 60% of the observations 
but only if that variable is insignificant.Taking this into consideration, 
imputation is always a preferred choice over deleting variables.






### Data manipulation #############################################################################

- df.method(inplace = True)				# Applied any method in df with inplace=True will change the original df
						# Initially try to use any method without inplace and see the result


- df.fillna('MISSING')					# Replace NaN values by MISSING string

- df.fillna(0)					# Replace NaN values by 0(zero)

- df['col'].replace(['zero', 'fifty'], [0, 50])			# Replace single/multiple string by any values 

- df.replace(['NA', 'Missing'], np.nan, inplace=True)		# Replace custom missing values by np.nan.  # System can deal with np.nan. It will consider as missing value

					
- df['col'] = df['col'].map({'Yes': True, 'No': False})		# Replacing/Mapping any values of a column # inplace argument does not work here. So explicitly need to assign changes 
				 
				

- df.loc[2] = ['v1', 'v2', 'v3']				# Update all values in row number 2

- df.loc[2, ['last', 'email']] = ['John', 'JohnDoe@email.com']	# Update only two column's value in row 2
				

- df.at[2, 'last'] = 'Doe'   				# Same as loc 



### Data observaton ###############################################################################

- df.describe()					# count, mean, std, min, 25%, 50%, 75%, max

- df.groupy('class').size()				# Display class count, On classifcation problems you need to know how balanced the class values are.

- df.corr(method='pearson')				# Pairwise correlation, # Highly correlated data can suffer in linear and logistic regression for classification
				
 
- ax.matshow(correlation, vmin=-1, vmax=1)			# visualize pairwise correlation matrix with color code

				
- df.skew()						# Many ML algorithms assume gaussian distriubution(normal, bell). 
						# An attribute has a skew may allow you to perform data preparation
						# to correct the skew and later improve the accuracy of your models.

- df.hist()						# Visualize an attribute is Gaussian, skewed or even has an exponential distribution 
						# It can also help you see possible outliers.

- df.plot(kind='density', subplots=True, layout=(3,3), sharex=False)
						# Abstracted histogram with a smooth curve drawn through the top of each bin
						# Visualize skewness of each attribute 

- df.plot(kind='box', subplots=True, layout=(3,3), sharex=False, sharey=False)
						# Observe the spread of the attributes 

- scatter_matrix(df)					# Pairwise relationship with scatter plot 





### Update rows and columns #######################################################################

- df.iloc[[0, 1], [2, 3]]					# iloc: integer location, string index will not work; 
						# Accessing col2 and col3 for row0 and row1

- df.loc[[0, 1], ['email', 'last']]				# loc: location; it works for string index such as email and last. email and last are two different columns

- df.loc[0:1, 'first':'email']				# Slicing works; but do not use bracket to make list

- df.loc['a@email.com', 'name']				# Row index: a@email.com, col index: name 



- df.columns					# Display all columns 

- df.columns = [x.upper() for x in df.columns]		# Change column name to upper case
- df.columns = [x.lower() for x in df.columns]		# Change column name to lower case 

- df.columns = df.columns.str.replace(' ', '_')			# Replace space in between column name  by '_'

- df.rename(columns={'c1': 'n1', 'c2': 'n2'}) 			# Rename column's name

- df['col'] = df['col'].astype('float')				# Type casting for a col, NaN values are float type under the hood

- df.astype('float')					# Type casting for whole df

- df.set_index('col', inplace=True)  			# Setting specific column as index. It can be done during loading data 



- df['full_name'] = df['first'] + ' ' + df['last']			# Adding two column and assigning to a new column

- df[['first', 'last']] = df['full_name'].str.split(' ', expand=True)	# Split a column by space and make two different columns 

- df.append({'col_1': 'V1', 'col_2': 'V2'}, ignore_index=True)              # Add a row; ignore_index=True automatically add new index for the new row 

- df = df.append(df2, ignore_index=True)			# Adding two df; inplace is not applicable so assignment is required 

- df.drop(columns=['first', 'last'], inplace=True)		# Delete two columns named last and first

- df.drop(index=4, inplace=True)				# Delete row which has index 4

- filt = (df['last'] == 'Doe')				# Delete rows where last column containing Doe 
  df.drop(index = df[filt].index)
						 



					
### Data filtering ################################################################################

- filt = (df['email'] == 'JohnDoe@email.com') 
  df.loc[filt, 'last'] = 'Smith' 				# Filtered and updated value 

- filt = (df['last'] == 'Doe') & (df['first'] == 'John')
 df.loc[filt, 'email']

- filt = (df['last'] == 'Schafer') | (df['first'] == 'John')
 df.loc[~filt, 'email']					# Negatively filtered with tilda 

- filt = (df['last'] == 'Doe')				# Delete rows where last column containing Doe 
  df.drop(index = df[filt].index)







### Aggregation and Grouping ####################################################################

- # Look at github/python/pandas/aggregate_grouping.ipynb

- AGGREGATION 

- df['salary'].median()					# Median of a sereies. Calculate median and ignore NaN values

- df.median()					# Median of a df. Calculate median of each column 

- df['salary'].count()					# How many people answered the question; Do not get confuse with value_counts()

- df['Hobbyist'].value_counts(normalize=True)		# How many people have answered yes how many people answered no; Normalze will give percentage 



-GROUPING

- country_grp = df.groupby(['Country']) 			# country_grp holds a df group object 

- country_grp.get_group('United States')			# Display results from specific group 

- country_grp['SocialMedia'].value_counts(normalize=True)	# Display multiple series such as country social media and counts

- country_grp['SocialMedia'].value_counts().loc['India']   	# Now only changing the country name you can search social media uses of each country

- filt = (df['Country'] == 'United States')			# Same result as group. But you have to filter for every country
  df.loc[filt]
  df.loc[filt, 'SocialMedia'].value_counts() 

- country_grp['salary'].median() 				# It returns median of each group 

- country_grp['salary'].median().loc['Germany']

- country_grp['salary'].agg(['median', 'mean'])		# Multiple aggregate function 

- country_grp['ConvertedComp'].agg(['median', 'mean']).loc['Finland']


- country_respondents = df['Country'].value_counts() 
- country_uses_python = country_grp['LanguageWorkedWith'].apply(lambda x: x.str.contains('Python').sum())
- python_df = pd.concat([country_respondents, country_uses_python], axis='columns')





### Data sorting ################################################################################

- df.sort_values(by='last', ascending=True)			# Sort by last name in ascending order 

- df.sort_values(by=['last', 'first'], ascending=False)		# Initially sort by last name then sort by first name in descending order

- df.sort_values(by=['last', 'first'], ascending=[False, True], inplace=True)

- df.sort_index(inplace=True)				# Sort by index 

- df['last'].sort_values()				# Sort a column named last 





### Apply, Map, Applymap, Replace #################################################################

# Look at github/python/pandas/pandas_and_dict.ipynb

- df['email'].apply(len)				# Custom function can be applied instead of len function

- df['email'] = df['email'].apply(lambda x: x.lower()) 		# Change each values of the column to lower case

- df.apply(len, axis='columns') 				# It will return length of the column of the dataframe 

- df.apply(pd.Series.min)				# It will return minimum values of each column

- df.apply(lambda x: x.min()) 				# x takes series object and find minimum value from it 



- df.applymap(len) 					# To dupdate whole dataframe. Applymap works on each value of the dataframe 

- df.applymap(str.lower) 				# All the value will be converted to lower case 



- df['first'].map({'Corey': 'Chris', 'Jane': 'Mary'})		# To update whole series; if you do not insert some values, it will be NaN



- df['first'].replace({'Corey': 'Chris', 'Jane': 'Mary'})		# Replace the specific value; Rest of the value will be unchanged 





### Data preparation ##############################################################################

Generally, I would recommend creating many dierent views and transforms of your data,
then exercise a handful of algorithms on each view of your dataset. This will help you to 
fush out which data transforms might be better at exposing the structure of your problem in general.




	