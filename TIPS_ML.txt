DATASET
https://github.com/jbrownlee/Datasets

MISSING VALUE 
----------------------------------------------------------------------------------------------------------------------------
-k-Nearest Neighbors that can ignore a column from a distance measure when a value is missing. 
Naive Bayes can also support missing values when making a prediction.

IMPUTE MISSING VALUE

-A constant value that has meaning within the domain, such as 0, distinct from all other values.
-A value from another randomly selected record.
-A mean, median or mode value for the column.
-A value estimated by another predictive model.

-Any imputing performed on the training dataset will have to be performed on new data in the 
future when predictions are needed from the finalized model. This needs to be taken into 
consideration when choosing how to impute the missing values. For example, if you choose 
to impute with mean column values, these mean column values will need to be stored to file 
for later use on new data that has missing values.



PREPARE DATA
----------------------------------------------------------------------------------------------------------------------------
https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing

Generally, I would recommend creating many dierent views and transforms of your data,
then exercise a handful of algorithms on each view of your dataset. This will help you to 
fush out which data transforms might be better at exposing the structure of your problem in general.


- RESCALE DATA

useful for algorithms that weight inputs like regression and neural networks and algorithms that 
use distance measures like k-Nearest Neighbors.


- STANDARDIZE

It is most suitable for techniques that assume a Gaussian distribution in the input variables and 
work better with rescaled data, such as linear regression, logistic regression and linear discriminate analysis.


- NORMALIZE

This pre-processing method can be useful for sparse datasets (lots of zeros) with attributes of varying 
scales when using algorithms that weight input values such as neural networks and algorithms that use
distance measures such as k-Nearest Neighbors.


- BINARIZE 

It can be useful when you have probabilities that you want to make into crisp values. It is also useful 
when feature engineering and you want to add new features that indicate something meaningful.



FEATURE SELECTION
----------------------------------------------------------------------------------------------------------------------------
https://scikit-learn.org/stable/modules/feature_selection.html

- Benifits: 1. Reduces overfitting 2. Improves accuracy 3. Reduce training time 

- UNIVARIATE SELECTION 

Many dierent statistical test scan be used with this selection method. For example the ANOVA F-value
method is appropriate for numerical inputs and categorical data

- RECURSIVE FEATURE ELIMINATION 

The Recursive Feature Elimination (or RFE) works by recursively removing attributes and
building a model on those attributes that remain. It uses the model accuracy to identify which
attributes (and combination of attributes) contribute the most to predicting the target attribute.

- PRINCIPLE COMPONENT ANALYSIS

Principal Component Analysis (or PCA) uses linear algebra to transform the dataset into a
compressed form. Generally this is called a data reduction technique. A property of PCA is that
you can choose the number of dimensions or principal components in the transformed result.

- FEATURE IMPORTANCE  

Bagged decision trees like Random Forest and Extra Trees can be used to estimate the importance
of features.



PERFORMENCE EVALUATION OF ML ALGORITHM
----------------------------------------------------------------------------------------------------------------------------

- TRAIN AND TEST SETS

This algorithm evaluation technique is very fast. It is ideal for large datasets (millions of
records) where there is strong evidence that both splits of the data are representative of the
underlying problem. Because of the speed, it is useful to use this approach when the algorithm
you are investigating is slow to train. A downside of this technique is that it can have a high
variance in results.

- K-FOLD CROSS VALIDATION

It estimate the performance of a ML algorithm with less variance than a single train-test set split.
It works by splitting the dataset into k-parts (e.g. k = 5 or k = 10).
Each split of the data is called a fold. The algorithm is trained on k - 1 folds with one held back 
and tested on the held back fold. This is repeated so that each fold of the dataset is given a 
chance to be the held back test set. After running cross-validation you end up with k different 
performance scores that you can summarize using a mean and a standard deviation.

- LEAVE ONE OUT CROSS VALIDATION

Leave one out cross validation computationally expensive than K-fold cross validation. 
It validates model for each data point. So it train the model except the one data point and tested with 
that data point only. It would give more variance than k-fold cross validation. 

- REPEATED RANDOM TEST TRAIN SPLITS 

It repeats the train-test split multiple times to reduce the variance or improve the performence. 
A down side is that repetitions may include much of the same data in the train or the test split 
from run to run, introducing redundancy into the evaluation. For example splits the data 
into a 67%/33% train/test split and repeats the process 10 times.

-REMARKS

- Generally k-fold cross-validation is the gold standard for evaluating the performance of a
machine learning algorithm on unseen data with k set to 3, 5, or 10.

- Using a train/test split is good for speed when using a slow algorithm and produces
performance estimates with lower bias when using large datasets.
 
- Techniques like leave-one-out cross-validation and repeated random splits can be useful
intermediates when trying to balance variance in the estimated performance, model
training speed and dataset size.

- The best advice is to experiment and find a technique for your problem that is fast and
produces reasonable estimates of performance that you can use to make decisions. If in doubt,
use 10-fold cross-validation.




PERFORMENCE METRICS
----------------------------------------------------------------------------------------------------------------------------

- CLASSIFICATION ACCURACY

Ratio of correct predictions and all predictions made. 
It is really suitable when there are equal number of observations in each class and that all predictions
and prediction errors are equally important. 


- LOGISTIC LOSS

Logistic loss (or logloss) is a performance metric for evaluating the predictions of probabilities of
membership to a given class. The scalar probability between 0 and 1 can be seen as a measure
of confidence for a prediction by an algorithm. Predictions that are correct or incorrect are
rewarded or punished proportionally to the confidence of the prediction.
Smaller logloss is better. 


- AREA UNDER ROC CURVE

https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc

The AUC represents a model's ability to discriminate between positive and negative
classes. 
A ROC Curve is a plot of the true positive rate and the false positive rate for a given set of 
probability predictions at dierent thresholds used to map the probabilities to class labels.


- CONFUSION MATRIX
            Act/Pre	1	0
	1	TP	FN	-> P
	0	FP	TN	-> N


- CLASSIFICATION REPORT

The classification report() function displays the precision, recall, F1-score and support for each
class.

Confusion matrix: 
            Act/Pre	1	0
	1	TP	FN	-> P
	0	FP	TN	-> N

Precision tells what is the percentage of true positive results out of the total predicted positive by the model.
Precision = TP/(TP+FP)

Recall tells what is the percentage of true positive results out of the actual positive values. 
Recall = TP/P

If a model classify one class much better than other class then F1 score plays role. It makes balance 
between precision and recall. 
F1 score = 2 * (Precision * Recall)/(Precision+Recall)


REGRESSION METRICS

- MEAN ABSOLUTE ERROR

The Mean Absolute Error (or MAE) is the sum of the absolute dierences between predictions
and actual values. It gives an idea of how wrong the predictions were. The measure gives an
idea of the magnitude of the error, but no idea of the direction

- MEAN SQUARED ERROR

The Mean Squared Error (or MSE) is much like the mean absolute error in that it provides a
gross idea of the magnitude of error.

This metric too is inverted so that the results are increasing. Remember to take the absolute
value before taking the square root if you are interested in calculating the RMSE.


- R SQUARE 

- Look at statistics.txt 

Quantifying the difference between the fitted line with the mean line.
That means calculating R(square). R(sqr) lies between 0 and 1 or in percentage.

for example: if size and weight relationship R(sqr) is .81, then it tells there is 
81% less variation around the line than the mean.


SPOT-CHECK CLASSIFICATION ALGORITHMS
----------------------------------------------------------------------------------------------------------------------------
It checks which algorithm perform well on your problem. 



COMPARING RESULTS OF MULTIPE ALGORITHMS
----------------------------------------------------------------------------------------------------------------------------

The key to a fair comparison of machine learning algorithms is ensuring that each algorithm is
evaluated in the same way on the same data. You can achieve this by forcing each algorithm
to be evaluated on a consistent test harness. 

 Logistic Regression.			-Linear algorithm
 Linear Discriminant Analysis.		-Linear algorithm
 k-Nearest Neighbors.			-Non-linear algorithm 
 Classication and Regression Trees.	-Non-linear algorithm
 Naive Bayes.			-Non-linear algorithm
 Support Vector Machines.		-Non-linear algorithm



WORKFLOWS WITH PIPELINES
----------------------------------------------------------------------------------------------------------------------------

- Use pipelines to minimize data leakage
- Construct data preperation and modeling pipeline
- Feature extraction and modeling pipeline 

DATA LEAKAGE

- preparing your data using normalization or standardization on the entire training dataset before 
learning would not be a valid test because the training dataset would have been in uenced by the 
scale of the data in the test set.

- Feature extraction is another procedure that is susceptible to data leakage. Like data preparation,
feature extraction procedures must be restricted to the data in your training dataset. 
Pipeline extract features from training set only. 
a handy tool called the FeatureUnion which allows the results of multiple feature selection and 
extraction procedures to be combined into a larger dataset on which a model can be trained.



IMPROVE PERFORMANCE WITH EMSEMBLES
----------------------------------------------------------------------------------------------------------------------------

- Ensembles can give you a boost in accuracy on your dataset.

The three most popular methods for combining the predictions from different models are:

- Bootstrap aggregation (Bagging): Building multiple models (typically of the same type) from 
different subsamples of the training dataset. 
Ex: Bagged decision trees, random forest, extra trees
-Bagging performs best with algorithms that have high variance.
 
- Boosting: Building multiple models (typically of the same type) each of which learns to
fix the prediction errors of a prior model in the sequence of models.
 
- Voting: Building multiple models (typically of differing types) and simple statistics (like
calculating the mean) are used to combine predictions.



ALGORITHM PARAMETERS TUNING/HYPERPARAMETER OPTIMIZATION 
----------------------------------------------------------------------------------------------------------------------------

- Grid search parameter tuning
- Random search parameter tuning



FINALIZING MODEL
----------------------------------------------------------------------------------------------------------------------------

- Keep note of python version
- Keep note of all library version 
- Keep note of hypermarameter used for the model. It can be used later in other model. 



PROJECT TEMPLATE
----------------------------------------------------------------------------------------------------------------------------
Python Project Template
# 1. Prepare Problem
# a) Load libraries
# b) Load dataset

# 2. Summarize Data
# a) Descriptive statistics
# b) Data visualizations

# 3. Prepare Data
# a) Data Cleaning
# b) Feature Selection
# c) Data Transforms

# 4. Evaluate Algorithms
# a) Split-out validation dataset
# b) Test options and evaluation metric
# c) Spot Check Algorithms
# d) Compare Algorithms

# 5. Improve Accuracy
# a) Algorithm Tuning
# b) Ensembles

# 6. Finalize Model
# a) Predictions on validation dataset
# b) Create standalone model on entire training dataset
# c) Save model for later use